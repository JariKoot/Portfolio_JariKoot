[["index.html", "Portofolio Book Introduction", " Portofolio Book Jari Koot 2021-08-26 Introduction Hey there! I am Jari, a 21 year old Life Science student. With a passion for programming and biology. This is my bookdown portfolio and it is written in Rmarkdown. You can read this book by clicking on the Knit button or visit my website via this link In this portfolio I will show you the skills I have learned during my DataScience for Biology minor. If you want to see all the files I created for this book than I recommend to visit my portfolio gihub page. "],["elegans-experiment.html", "# Elegans experiment", " # Elegans experiment About the exercise In this exercise I have imported a dataset in Rstudio and created a scatterplot. During the exercise i encountered some problems with the data, the goal of this exercise is to fix the errors that occur and create a nice looking plot. The exercise teaches me to reproduce data analysis from somebody elses work. A Review the following Excel file in the ./data/CE.LIQ.FLOW.062_Tidydata.xlsx (its here), by opening the file in Excel. See if you can spot anything peculiar about this file. Do not edit the file in any way. Just close it when you are done. (Annoyingly, Excel asks you to save your changes, even if you did not touch anything in the file: why is this cumbersome?) There are a lot of different colours used, this makes it hard to read the data. Also the expData is a column without data. B Open the file in R, using the {readxl} package. elegans_data &lt;- read_excel(here(&quot;data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) elegans_data %&gt;% head(10) ## # A tibble: 10 x 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName expDate expResearcher expTime expUnit expVolumeCounted RawData ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NA NA 1 a experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 44 ## 2 NA NA 1 b experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 37 ## 3 NA NA 1 c experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 45 ## 4 NA NA 1 d experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 47 ## 5 NA NA 1 e experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 41 ## 6 NA NA 2 a experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 35 ## 7 NA NA 2 b experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 41 ## 8 NA NA 2 c experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 36 ## 9 NA NA 2 d experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 40 ## 10 NA NA 2 e experi~ 3 CE.LIQ~ 2020-11-30 00:00:00 Sergio Reijn~ 68 hour 50 38 ## # ... with 21 more variables: compCASRN &lt;chr&gt;, compName &lt;chr&gt;, compConcentration &lt;chr&gt;, compUnit &lt;chr&gt;, compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, ## # elegansStrain &lt;chr&gt;, elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, ## # bacterialVolume &lt;dbl&gt;, bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, incubationUnit &lt;chr&gt;, incubationMethod &lt;chr&gt;, ## # incubationRPM &lt;dbl&gt;, bubble &lt;lgl&gt;, incubateTemperature &lt;dbl&gt; C Inspect the data types of columns RawData, compName and compConcentration. What types would you expect from the experimental description above. Have the data types been correctly assigned during the importing of the data into R? The compConcentration is a character column and you would expect an numeric column, the data is imported incorrectly D Create a graph displaying a scatterplot for the CE.LIQ.FLOW.062_Tidydata.xlsx data, for the different compounds and the varying concentrations. Put the compConcentration on the x-axis, the DataRaw counts on the y-axis and assign a colour to each level in compName. Assign a different symbol (shape =) to each level in the expType variable. Try fixing the labels of the x-axis so that we can read them. E When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens. Look at the data-type of the compConcentration column in the data again to find a clue. is.numeric(elegans_data$compConcentration) ## [1] FALSE The data is not numeric, this means that every concentration is seen as a separate point. Next we will make the data numeric and also add a column with compConcentration to put on the x-as. F Correct the data-type of compConcentration to numeric and than look at the graph again. Use a log10 transformation on the x-axis to get a clear graph. Also, add a bit of jitter to the points in the graph so that points are not overlapping. #make the data numeric elegans_filter$compConcentration &lt;- as.numeric(elegans_filter$compConcentration) #add a column with the log of the concentration elegans_filter_log &lt;- elegans_filter %&gt;% mutate(conc_log = log(elegans_filter$compConcentration)) #make a graph ggplot(data = elegans_filter_log, aes(x = conc_log, y = RawData)) + geom_jitter(aes(colour = compName, shape = expType), alpha = 0.7, size =1) + labs(x = &quot;compound concentration&quot;, y = &quot;Offspring&quot;, title = &quot;The amount of offspring after incubation with compound&quot;) G &amp; H The positive control for this experiments is ethanol. (H) The negative control for this experiment is S-medium. I Think about how you would analyze this experiment to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50). Write down you analysis as a step-wise plan To prove that there is a significant difference between the positive control and the samples we first need to preform a shapiro test. Than we perform a ANOVA test. J Normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and that all other values are expressed as a fraction thereof. Rerun your graphs with the normalized data. #calculate the mean RawData of s-medium mean_elegans_data &lt;- elegans_data %&gt;% select(RawData, compName, compConcentration) %&gt;% filter(compName == &#39;S-medium&#39;) %&gt;% group_by(compName) %&gt;% summarise(mean_RawData = mean(RawData)) #express all the values to a fraction of the normalized data normalized_elegans_data &lt;- elegans_data %&gt;% select(compName, compConcentration, RawData) %&gt;% mutate(Normalized_RawData = RawData / mean_elegans_data$mean_RawData) mean_normalized_elegans_data &lt;- normalized_elegans_data %&gt;% group_by(compName, compConcentration) %&gt;% summarise(mean_normalized_RawData = mean(Normalized_RawData)) mean_normalized_elegans_data ## # A tibble: 22 x 3 ## # Groups: compName [5] ## compName compConcentration mean_normalized_RawData ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2,6-diisopropylnaphthalene 0.499 0.504 ## 2 2,6-diisopropylnaphthalene 4.9899999999999996E-3 0.875 ## 3 2,6-diisopropylnaphthalene 4.9899999999999999E-4 0.967 ## 4 2,6-diisopropylnaphthalene 4.99 0.470 ## 5 2,6-diisopropylnaphthalene 4.99E-2 0.687 ## 6 2,6-diisopropylnaphthalene 4.99E-5 1.04 ## 7 decane 0.499 0.257 ## 8 decane 4.9899999999999996E-3 1.07 ## 9 decane 4.9899999999999999E-4 1.08 ## 10 decane 4.99 0.238 ## # ... with 12 more rows mean_normalized_elegans_data %&gt;% ggplot(aes(x = compConcentration, y = mean_normalized_RawData)) + geom_point() + geom_jitter(aes(colour = compName, shape = compName)) + labs(x = &quot;compound concentration&quot;, y = &quot;Offspring&quot;, title = &quot;The amount of offspring after incubation with compound&quot;) *** K Why would you want to take the step under J? This makes it easier to compare the samples to the normal value. "],["review-of-an-article.html", "# Review of an article", " # Review of an article About the exercise In this exercise I reviewed a scientific publication from Research Square. The goal is to control the articel on reproducibility and completeness. A-D Initiate an empty RMarkdown file in your RStudio environment and provide author and title (after the title of this exercise) Search for a primary Open Access article on one of the above listed topics, using Pubmed Central Read the article diagonally to check if is indeed a primary article describing emperical scientific findings. Include the reference to this article in your Rmd file The article is found here The data of the article is found here Its an open access article and it is a primary article. E Score the article on the basis of the above Repita criteria: Repita Yes / No Study purpose yes Data availability statement yes Study location yes author review yes ethics statements yes funding statement yes code availability yes F Write an Rmarkdown report on your findings, including the table above and some information about the article such as general aim, short methods and results. If data is available, try including some R markdown report The used article scores good on all the criteria of the Repita points. Purpose of the study The study is done to give more insight in the problems that occure in the healthcare section during the COVID-19 pandemic. In the study the hypothesis is that the levels of anxiety would be higher in healtcare workers from a region with the highest rates of COVID-19 cases as compared tot hose from a region with the lowest rates of these cases. Procedure The data from the study was colleted online via surveys, emails and various social networks. The first survey was taken from april 2 to may 30, 2020. the follow up was performed during september 15th to 30th 2020, participants were contacted to answer the online survey for the second time. Results In the entire sample, 56.07% of healthcare workers scored as high anxiety in the rst measurement, and signicantly increased to 65.57% in the follow-up. There was a positive and signicant relationship between the levels of anxiety in the two measurements. The mean level of anxiety in the first measurement was 31.97 (s.d. ± 11.28), while in the follow-up the mean signicantly increased to 34.41 (s.d. ± 12.97). G-I Store the source Rmd and knitted HTML in a folder called Rmd in your course RStudio project. You will need it again later in the course Done The script of the article is available in the R script: les1/portfolio1.2script.R J Have a look at the code. Describe in your own words what the code intents to achieve. The code processes the data that was collected via the online surveys. To test if the anxiety levels differ significantly there are multiple t-test performed. The code also makes plots using the ggplot function. K In terms of readibility of the code, how would you grade (1(very bad)-5(very good)) the code available. I would rate the script an 3 for readibility L&amp;M Done N When you encounter errors or flaws in the script, try fixing them and record your changes. The code that didnt work at many placed the dataset has other column names that are mentioned in the code. This makes it inpossible for me to correct the data without the correct dataset :( O Taken together on a scale from 1 (very hard) to 5 (very easy), how much effort did it take you to reproduce the visualization from the project, report or article I rate the code an 1. It took me a long time to try to reproduce the code. This was nearly impossible because the needed files were not provided on the GitHub page. "],["guerrilla-analytics.html", "# Guerrilla analytics", " # Guerrilla analytics About the exercise During this exercise I learned about the Guerrilla Analytics Principles. I will add a proper folder structure to all my projects. The folder structure is shown below. A Look at your RStudio project that you created for the DAUR-II final assignment Rearrange your project according the Guerilla principles explained above Add README files to the datasets Use the {fs} package to share a screenshot of your folder tree in your portfolio, look here for more info on how to use the {fs} package. library(fs) dir_tree(path = &quot;.&quot;) ## . ## +-- 001_elegans_experiment.Rmd ## +-- 002_review_article.Rmd ## +-- 003_guerrilla_analytics.Rmd ## +-- 004_CV_JariKoot.Rmd ## +-- 005_looking_ahead.Rmd ## +-- 006_new_skills.Rmd ## +-- 007_relational_databases.Rmd ## +-- 008_COVID_19_parameterization.Rmd ## +-- 008_COVID_19_parameterization_files ## | \\-- figure-html ## | +-- covid cases-1.png ## | \\-- covid deaths-1.png ## +-- app ## | \\-- waternet_app.R ## +-- code ## | +-- Microbiota_analysis.R ## | \\-- portfolio1.2script.R ## +-- data ## | +-- CV_Jari.html ## | +-- CV_Jari.pdf ## | +-- CV_Jari_files ## | | +-- font-awesome-5.1.0 ## | | | +-- css ## | | | | +-- all.css ## | | | | \\-- v4-shims.css ## | | | \\-- webfonts ## | | | +-- fa-brands-400.eot ## | | | +-- fa-brands-400.svg ## | | | +-- fa-brands-400.ttf ## | | | +-- fa-brands-400.woff ## | | | +-- fa-brands-400.woff2 ## | | | +-- fa-regular-400.eot ## | | | +-- fa-regular-400.svg ## | | | +-- fa-regular-400.ttf ## | | | +-- fa-regular-400.woff ## | | | +-- fa-regular-400.woff2 ## | | | +-- fa-solid-900.eot ## | | | +-- fa-solid-900.svg ## | | | +-- fa-solid-900.ttf ## | | | +-- fa-solid-900.woff ## | | | \\-- fa-solid-900.woff2 ## | | +-- header-attrs-2.7 ## | | | \\-- header-attrs.js ## | | \\-- paged-0.14 ## | | +-- css ## | | | \\-- resume.css ## | | \\-- js ## | | +-- config.js ## | | +-- hooks.js ## | | \\-- paged.js ## | +-- data.csv ## | +-- dengue_data.csv ## | +-- dengue_data.rds ## | +-- dengue_data_equal.csv ## | +-- dengue_data_equal.rds ## | +-- example.final.an.unique_list.0.03.cons.taxonomy ## | +-- example.final.an.unique_list.0.03.norm.shared ## | +-- example.final.an.unique_list.shared ## | +-- example.metadata.txt ## | +-- example.SCFA.txt ## | +-- flu_data.csv ## | +-- flu_data.rds ## | +-- flu_data_equal.csv ## | +-- flu_data_equal.rds ## | +-- gapminder.csv ## | \\-- gapminder.rds ## +-- data_raw ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- dataset.xlsx ## | +-- output_blast_post ## | \\-- output_blast_pre ## +-- images ## | +-- JariKoot_pf.jpg ## | \\-- shiny_sketch.jpg ## +-- index.Rmd ## +-- LICENSE ## +-- Portfolio_JariKoot_Herkansing.Rproj ## +-- README.md ## +-- search_index.json ## +-- _book ## | +-- covid-19-parameterized-report.html ## | +-- data ## | | \\-- CV_jari.pdf ## | +-- elegans-experiment.html ## | +-- guerrilla-analytics.html ## | +-- images ## | | \\-- shiny_sketch.jpg ## | +-- index.html ## | +-- jaris-cv.html ## | +-- libs ## | | +-- anchor-sections-1.0.1 ## | | | +-- anchor-sections.css ## | | | \\-- anchor-sections.js ## | | +-- gitbook-2.6.7 ## | | | +-- css ## | | | | +-- fontawesome ## | | | | | \\-- fontawesome-webfont.ttf ## | | | | +-- plugin-bookdown.css ## | | | | +-- plugin-clipboard.css ## | | | | +-- plugin-fontsettings.css ## | | | | +-- plugin-highlight.css ## | | | | +-- plugin-search.css ## | | | | +-- plugin-table.css ## | | | | \\-- style.css ## | | | \\-- js ## | | | +-- app.min.js ## | | | +-- clipboard.min.js ## | | | +-- jquery.highlight.js ## | | | +-- lunr.js ## | | | +-- plugin-bookdown.js ## | | | +-- plugin-clipboard.js ## | | | +-- plugin-fontsettings.js ## | | | +-- plugin-search.js ## | | | \\-- plugin-sharing.js ## | | +-- header-attrs-2.8 ## | | | \\-- header-attrs.js ## | | \\-- jquery-2.2.3 ## | | \\-- jquery.min.js ## | +-- looking-ahead.html ## | +-- new-skills.html ## | +-- relational-databases.html ## | +-- review-of-an-article.html ## | +-- search_index.json ## | +-- _book ## | | \\-- _main_files ## | | \\-- figure-html ## | | +-- Relational_databases_plot1.png ## | | +-- Relational_databases_plot2.png ## | | \\-- Relational_databases_plot3.png ## | \\-- _main_files ## | \\-- figure-html ## | +-- correct data-1.png ## | +-- covid cases-1.png ## | +-- covid deaths-1.png ## | +-- graph normalized-1.png ## | +-- graph-1.png ## | +-- Relational_databases_plot1.png ## | +-- Relational_databases_plot2.png ## | \\-- Relational_databases_plot3.png ## +-- _bookdown_files ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html ## +-- correct data-1.png ## +-- covid cases-1.png ## +-- covid deaths-1.png ## +-- graph normalized-1.png ## \\-- graph-1.png "],["jaris-cv.html", "# Jaris CV", " # Jaris CV About the exercise In this exercise I created a CV with Rmarkdown. I am using a template that is available on this GitHub page You can find my CV by clicking on this link "],["looking-ahead.html", "# Looking ahead", " # Looking ahead About the exercise In this exercise the goal is to look ahead in the future. Where do I see myself in a few years and how am I going to reach that goal? Exercise looking ahead In class, we discussed this assignment yesterday. In the following weeks, start looking for what you will want to spend your extra time on this semester. Try, for yourself, to answer the following questions: Where do I want to be in ~2 years time? How am I doing now with respect to this goal? What would be the next skill to learn? Make a planning on how to start learning this new skill. My future plans Where do I want to be (in 2 years)? I thought about this for a while and I was thinking back to all the school projects I did in the past few years. It came to my attention that microbiological research was my favorite project. In this project I had to take several water samples from the my school and analyse these samples on there microbiota. I liked this project a lot because you can create some kind of visualization of things that are actually invisible to our eyes. You can only really see what is in the water if you do microbiological research. Thats why this kind of research interests me and I would like to do something with it in the future How am I going to reach this goal? Im looking online for places that do this kind of research. I prefer a internship where they work with large data sets so that i can apply the DataSciences minor in my work. And if the internship suits me well, I see myself going to work at such a company in the future. What would be the next skill to learn? It is important in this research area that you can analyze and visualize the data properly. I found a workflow that explains the basics of analysing microbiota in R (page found here). They explain how you can manipulate data and visualize this in different types of plots. It would be very useful to view these types of plots interactively. For example, that you can select how many reads will be visualized and how they are plotted. Therefore I am going to learn the basics of Shiny (page found here). I dont have to learn the whole Shiny package in detail. For me its about learning the basics of Shiny. Make a planning on how to start learning this new skill. The teacher gave us 4 days to expand our knowledge in subjects of our choice. I have decided to divide these 4 days into 2x2 days. in the first 2 days I will do a basic microbiota analysis. The last 2 days Im going to follow workflow about the basics of Shiny. "],["new-skills.html", "# New skills", " # New skills About the exercise In this Rmarkdown file I will explain the new skills I learned (this is mentioned in the previous chapter). The goal is to learn a new skill in 4 days of self study, I decided to divide my 4 days in to 2x2 days to learn the following skills: Microbiota analysis (link to the workflow) Shiny (link to the workflow) I came up with an assignment to test my newly learned skills, the assignment is to analyse a dataset (this dataset is from my last project) and apply some plots on the samples. After that I make the plots visible in a Shiny app. In the app I will ask for 3 different inputs that the user can adjust. Microbiota analysis The goal of this workflow is to learn basic analyses of microbiota data to determine if and how communities differ by variables of interest. This pipeline can be used for any microbiota dataset that has been clustered into operational taxonomic units: OTUs. All the resultes, code and pipeline are documented in the file: code/Microbiota_analysis.R. In this workflow I will learn about alpha and beta-diversity between and within groups. After that there are different kind of graphs introduced (Abundance plots, heat maps, etc.). Shiny The second skill I will learn about is Shiny. Shiny is a package that can be installed in Rstudio, it makes it easy to build interactive web apps straight from Rstudio. To show the plots you created in a project it is important that the figures are interpreted clearly and comprehensibly. Shiny gives you the opportunity to do this in an app with interactive plots. My goal of this workflow is to learn the basics of Shiny and make a small app that shows you the results of my last project in an interactive way. Below I will give you a quick introduction of my last project. Summary of my last project The data used in this workflow is from my last project. Here is a short summary of the project: The Waternet project is a project commissioned by Waternet. Its aim is to get insight in the bacterial composition of Waternets water, before and after filtering. The project was initially handled by a group op microbiology students. Both mock solutions and samples were used for determination, the latter as a control on the determination process. After concluding they had insufficient resources to properly determine the composition of the samples, the samples were sent to a company that performed whole genome sequencing on them. As a continuation of this project, a group of data science students (us) were assigned the task of coming up with a way of determining the bacterial composition of both samples, based on the WGS data. For more information about the project visit our GitHub page. The samples I am going to analyse are the first 10 000 reads of the pre and post sample (this will be ajustable via the interface). I am gonna visualize them in different ways so that the data will be understandable to interpret. If you are working with Shiny its a good idea to start with a sketch. In the image below you can see mine. Shiny app I decided to put both samples (pre and post) next to eachother in the app, so that you will get a quick overview what is different in each sample. I also made use of tabs so that the table and plots are not on the same page. This makes it easier to view the data. To start the app you need to open the file app/waternet_app.R and press on the button: Run App. The app is best viewed in full screen. "],["relational-databases.html", "# Relational databases", " # Relational databases About the exercise In this exercise I learned about SQL and database-skills. firstly, the data was imported from this GitHub page. After that the data is manipulated in such a way that the 3 datasets can be merged together. At last the data is visualized in 3 different plots. A Load the flu (./data/flu_data.csv), the dengue (./data/dengue_data.csv) and the gapminder ({dslabs} package) into three separate dataframes in R #Load the flu dataset flu_data_import &lt;- read.csv(&quot;data/flu_data.csv&quot;, skip = 11) #Load the dengue dataset dengue_data_import &lt;- read.csv(&quot;data/dengue_data.csv&quot;, skip = 11) #Load the gapminder dataset gapminder &lt;- dslabs::gapminder After the datasets were loaded I had to adjust the way the data is presented B Check if they are in the right shape. Is the data in the tidy format? If not change the format to tidy #Make flu_data tidy flu_data_tidy &lt;- flu_data_import %&gt;% tidyr::pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;value&quot;) head(flu_data_tidy) ## # A tibble: 6 x 3 ## Date country value ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Australia NA ## 3 2002-12-29 Austria NA ## 4 2002-12-29 Belgium NA ## 5 2002-12-29 Bolivia NA ## 6 2002-12-29 Brazil 174 #Make dengue_data tidy dengue_data_tidy &lt;- dengue_data_import %&gt;% tidyr::pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;value&quot;) head(dengue_data_tidy) ## # A tibble: 6 x 3 ## Date country value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Bolivia 0.101 ## 3 2002-12-29 Brazil 0.073 ## 4 2002-12-29 India 0.062 ## 5 2002-12-29 Indonesia 0.101 ## 6 2002-12-29 Mexico NA One of the first steps in analysing datasets is to make them tidy. Tidy datasets provide a way to link the structure of a dataset (the layout) with its semantics (what values are in the dataset). c Change the country and date variables of the three tables so that they coincide in terms of data type, class and values ##FLU DATASET #Seperate the flu date flu_data_equal &lt;- flu_data_tidy %&gt;% separate(&quot;Date&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) #Change the data type flu_data_equal$year &lt;- as.integer(flu_data_equal$year) flu_data_equal$country &lt;- as.factor(flu_data_equal$country) #Delete the day and month column flu_data_equal &lt;- flu_data_equal[-c(2,3)] head(flu_data_equal) ## # A tibble: 6 x 3 ## year country value ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2002 Argentina NA ## 2 2002 Australia NA ## 3 2002 Austria NA ## 4 2002 Belgium NA ## 5 2002 Bolivia NA ## 6 2002 Brazil 174 ##DENGUE DATASET #Seperate the dengue date dengue_data_equal &lt;- dengue_data_tidy %&gt;% separate(&quot;Date&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) #Change the data type dengue_data_equal$year &lt;- as.integer(dengue_data_equal$year) dengue_data_equal$country &lt;- as.factor(dengue_data_equal$country) #Delete the day and month column dengue_data_equal &lt;- dengue_data_equal[-c(2,3)] head(dengue_data_equal) ## # A tibble: 6 x 3 ## year country value ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2002 Argentina NA ## 2 2002 Bolivia 0.101 ## 3 2002 Brazil 0.073 ## 4 2002 India 0.062 ## 5 2002 Indonesia 0.101 ## 6 2002 Mexico NA As you can see, the datasets are similar in structure. This makes it easy to merge the datasets together. D Store the three tables as separate (so six in total) .csv and .rds files. #Store the files as .csv write.csv(flu_data_equal, &quot;data/flu_data_equal.csv&quot;, row.names = FALSE) write.csv(dengue_data_equal, &quot;data/dengue_data_equal.csv&quot;, row.names = FALSE) write.csv(gapminder, &quot;data/gapminder.csv&quot;, row.names = FALSE) #Store the files as .rds saveRDS(flu_data_equal, &quot;data/flu_data_equal.rds&quot;) saveRDS(dengue_data_equal, &quot;data/dengue_data_equal.rds&quot;) saveRDS(gapminder, &quot;data/gapminder.rds&quot;) E In Dbeaver create a new PostgreSQL database workflowsdb #I left out my password, therefore the code will not work. If you have Postgres you can fill in your own details. #Delete every, eval=FALSE from the headliners of the chunk. Than this Rmarkdown file will work. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;myfirstdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;xxx&quot;) For privacy reasons I left out my password, therefore the code will not work. If you have Postgres you can fill in your own details. After that delete every: eval=FALSE from the headliners of the chunk. Than this Rmarkdown file will work. F Using RPostgreSQL, insert the tables into the database. dbWriteTable(con, &quot;dengue_data_db&quot;, dengue_data_equal, overwrite = T) dbWriteTable(con, &quot;flu_data_db&quot;, flu_data_equal, overwrite = T) dbWriteTable(con, &quot;gapminder_db&quot;, gapminder, overwrite = T) After connecting to a PostgreSQL database (using the RPostgres package) I inserted the 3 different tables into one database. G Inspect the contents of the tables with SQL (in DBeaver) and save the SQL script. SELECT * FROM &quot;dengue_data_db&quot;; SELECT * FROM &quot;flu_data_db&quot;; SELECT * FROM &quot;gapminder_db&quot;; H Inspect the contents of the tables with dplyr (in R) and save a RMarkdown showing what you are doing. db_names &lt;- c(&quot;flu_data_db&quot;, &quot;dengue_data_db&quot;, &quot;gapminder_db&quot;) databases &lt;- list() for (x in db_names){ databases[[paste(x)]] &lt;- dbReadTable(con, x) } #inspect the data #databases[1] #databases[2] #databases[3] I Load the gapminder data in R and change the dataframe in such as way that you could join it to dengue and flue. #Filter the year of gapminder_db from 2002 to 2015, so it will fit the data format of dengue and flue gapminder_change &lt;- databases[3] %&gt;% as.data.frame() gapminder_filter &lt;- gapminder %&gt;% filter(between(year, 2002, 2015)) J Save this clean gapminder data in the workflowsdb database dbWriteTable(con, &quot;gapminder_filter&quot;, gapminder_filter, overwrite = T) K Perform some joins (your choice) with SQL (can be done in DBeaver or with dplyr. #Combine all the tables with left_join data_dengue_flu &lt;- left_join(dengue_data_equal, flu_data_equal, by = c(&quot;country&quot;, &quot;year&quot;)) data_all_tables &lt;- left_join(data_dengue_flu, gapminder_filter, by = c(&quot;country&quot;, &quot;year&quot;)) I chose to do perform the joins with dplyr. The data is joined on the variable country and year. L Generate a joined table, and export this from the database to R. The combined dataframe is already in R (called data_all_tables) M Show some descriptive statistics with this table, and at least 3 visualisations using ggplot2. #Plot 1, show the fertility over the past years in America fertility_america &lt;- data_all_tables %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% select(fertility, year) fertility_america #Filter countries on Brazil, Mexico and Bolivia data_3_countries &lt;- data_all_tables %&gt;% filter(country == c(&quot;Brazil&quot;, &quot;Mexico&quot;, &quot;Bolivia&quot;)) #Plot1, life expectancy in the continents ggplot(data_3_countries, aes(x = country, y = life_expectancy)) + geom_boxplot(aes(fill = country), outlier.colour = &quot;black&quot;) + labs(title = &quot;Boxplot of the life expectancy in Brazil, Mexico and Bolivia&quot;, y = &quot;life expectancy&quot;, x = &quot;&quot;) To show you the plots I made without sharing you my password, I have include the plots via knitr. Below you can find the 3 plots I have created with ggplot. In the first plot you will see a boxplot of the Life expectancy in Brazil, Mexico and Bolivia. In the second plot you see a bar graph of the flu data over the years in America. In the third plot you see a bar graph of the infant mortality in America over the past few years #Include the first plot knitr::include_graphics( here::here( &quot;_book&quot;, &quot;_main_files&quot;, &quot;figure-html&quot;, &quot;Relational_databases_plot1.png&quot; ) ) #plot 2, show the flu data over the past few years in the continent America flu_america &lt;- data_all_tables %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% select(value.x, year) ggplot(data=flu_america) + geom_col(aes(x = year, y = value.x, fill = year), show.legend = FALSE) + labs(title = &quot;Bar graph of the flu data over the years in America&quot;, y = &quot;&quot;) #Include the second plot knitr::include_graphics( here::here( &quot;_book&quot;, &quot;_main_files&quot;, &quot;figure-html&quot;, &quot;Relational_databases_plot2.png&quot; ) ) #plot 3, line graph of the dengue data in azia over the years dengue_America &lt;- data_all_tables %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% select(infant_mortality, year) ggplot(data = dengue_America) + geom_col(aes(x = year, y = infant_mortality, fill = year), show.legend = FALSE)+ scale_fill_gradient(low=&quot;red&quot;, high=&quot;yellow&quot;) + labs(title = &quot;Bar graph of the infant mortality in America over the past years&quot;, y = &quot;infant mortality&quot;) #Include the third plot knitr::include_graphics( here::here( &quot;_book&quot;, &quot;_main_files&quot;, &quot;figure-html&quot;, &quot;Relational_databases_plot3.png&quot; ) ) "],["covid-19-parameterized-report.html", "# COVID-19 parameterized report", " # COVID-19 parameterized report About the exercise In this exercise I will create a parameterized report of COVID-19 cases and deaths. The data used in this exercise can be found here. This data will be filtered on the different parameters, these are: Params Explanation Data The location of the file that should be used Country Which country to filter on Day Day of the first date to work with Month Month of the first date to work with Year Year of the first date to work with The params can be filled in in the index.Rmd. the first date that can be used is 2-3-2021. If you change a param type this in the console: bookdown::render_book(&quot;.&quot;) This makes it so that the .html files are reload and the params will be correct fully implemented #Set this option in the first code chunk knitr::opts_chunk$set(echo = params$printcode) #Loading the data given by a param COVID_data &lt;- read.csv(params$data) #filter on country given by a param COVID_country &lt;- COVID_data %&gt;% filter(countriesAndTerritories == params$country) #getting the rownumber of the given params (day, month, year) row_number &lt;- which((COVID_country$day == params$day) &amp; (COVID_country$month == params$month) &amp; (COVID_country$year == params$year)) #Filter the COVID_country from the first line to row_number COVID_filter &lt;- COVID_country[1:row_number, ] #replace date column to date class COVID_filter$dateRep &lt;- as.Date(COVID_filter$dateRep, format = &quot;%d/%m/%y&quot;) #Creating a variable with the country name country &lt;- params$country #Plotting the graph COVID_filter %&gt;% ggplot() + geom_line(aes(x = dateRep, y = cases), color = &quot;red&quot;) + theme_classic() + labs(title = paste(&quot;recorded COVID-19 cases in&quot;, country), x = &quot;&quot;) #Plotting the graph COVID_filter %&gt;% ggplot() + geom_line(aes(x = dateRep, y = deaths), color = &quot;red&quot;) + theme_classic() + labs(title = paste(&quot;Recorded COVID-19 deaths in&quot;, country), x = &quot;&quot;) "]]
